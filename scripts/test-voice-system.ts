/**
 * VOICE SYSTEM INTEGRATION TEST
 *
 * Prueba completa del sistema de voz multimodal
 */

import { PrismaClient } from "@prisma/client";
import { createEmotionalAgent } from "../lib/emotional-system/utils/initialization";
import { getWhisperClient } from "../lib/voice-system/whisper-client";
import { getElevenLabsClient } from "../lib/voice-system/elevenlabs-client";
import { getVoiceConfig } from "../lib/voice-system/voice-initialization";
import { getEmotionalSystemOrchestrator } from "../lib/emotional-system/orchestrator";
import fs from "fs";
import path from "path";

const prisma = new PrismaClient();

async function testVoiceSystem() {
  console.log("\nğŸ¤ TESTING VOICE SYSTEM - COMPLETE INTEGRATION");
  console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

  const startTime = Date.now();

  try {
    // 1. Setup test user
    console.log("1ï¸âƒ£  Setting up test user...");

    let testUser = await prisma.user.findUnique({
      where: { email: "test-voice@example.com" },
    });

    if (!testUser) {
      testUser = await prisma.user.create({
        data: {
          email: "test-voice@example.com",
          name: "Test User (Voice System)",
        },
      });
    }

    console.log(`   âœ… Test user ready: ${testUser.id}\n`);

    // 2. Create emotional agent with voice
    console.log("2ï¸âƒ£  Creating emotional agent with auto-selected voice...");

    const agentId = await createEmotionalAgent({
      userId: testUser.id,
      name: "Sofia",
      kind: "companion",
      preset: "warmCompanion",
      gender: "female",
      description:
        "Una compaÃ±era cÃ¡lida y empÃ¡tica de Argentina que valora las conexiones genuinas",
      accent: "es-AR",
      backstory:
        "Soy Sofia, una persona que disfruta las conversaciones profundas y crear lazos emocionales autÃ©nticos.",
      enableVoice: true,
    });

    console.log(`   âœ… Agent created: ${agentId}\n`);

    // 3. Verify voice configuration
    console.log("3ï¸âƒ£  Verifying voice configuration...");

    const voiceConfig = await getVoiceConfig(agentId);

    if (!voiceConfig) {
      throw new Error("Voice config not created!");
    }

    console.log(`   âœ… Voice configured:`);
    console.log(`      Voice ID: ${voiceConfig.voiceId}`);
    console.log(`      Voice Name: ${voiceConfig.voiceName}`);
    console.log(`      Gender: ${voiceConfig.gender}`);
    console.log(`      Accent: ${voiceConfig.accent || "none"}`);
    console.log(
      `      Selection Confidence: ${voiceConfig.selectionConfidence.toFixed(2)}`
    );
    console.log(`      Manual Selection: ${voiceConfig.manualSelection}\n`);

    // 4. Test ElevenLabs voice search
    console.log("4ï¸âƒ£  Testing ElevenLabs voice library search...");

    const elevenlabs = getElevenLabsClient();

    const femaleSpanishVoices = await elevenlabs.searchVoices({
      gender: "female",
      accent: "es",
    });

    console.log(
      `   âœ… Found ${femaleSpanishVoices.length} female Spanish voices in library\n`
    );

    // 5. Test voice generation (text to speech)
    console.log("5ï¸âƒ£  Testing voice generation (Text-to-Speech)...");

    const testMessage = "Â¡Hola! Â¿CÃ³mo estÃ¡s? Me alegra mucho poder hablar contigo.";

    const voiceResult = await elevenlabs.generateSpeech(
      testMessage,
      voiceConfig.voiceId,
      {
        currentEmotion: "joy",
        intensity: 0.6,
        mood: {
          valence: 0.5,
          arousal: 0.6,
          dominance: 0.5,
        },
        stability: 0.5,
        similarity_boost: 0.75,
      }
    );

    console.log(`   âœ… Voice generated:`);
    console.log(`      Audio size: ${voiceResult.audioBuffer.length} bytes`);
    console.log(`      Voice used: ${voiceResult.voiceName}\n`);

    // Guardar audio para verificaciÃ³n manual
    const outputDir = path.join(process.cwd(), "test-output");
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }

    const audioPath = path.join(outputDir, "test-voice-output.mp3");
    fs.writeFileSync(audioPath, voiceResult.audioBuffer);

    console.log(`   ğŸ’¾ Audio saved to: ${audioPath}\n`);

    // 6. Test Whisper transcription (si hay audio de prueba)
    console.log("6ï¸âƒ£  Testing Whisper transcription...");
    console.log(
      "   â„¹ï¸  Skipping (requires real audio file - manually test via API)\n"
    );

    // 7. Test complete voice interaction flow
    console.log("7ï¸âƒ£  Testing complete voice interaction flow...");

    const orchestrator = getEmotionalSystemOrchestrator();

    // Simular mensaje del usuario
    const userMessage = "Hola Sofia, hoy tuve un dÃ­a muy difÃ­cil en el trabajo";

    const response = await orchestrator.processMessage({
      agentId,
      userMessage,
      userId: testUser.id,
    });

    console.log(`   ğŸ‘¤ Usuario: "${userMessage}"`);
    console.log(`   ğŸ¤– Sofia: "${response.responseText}"\n`);

    // Generar audio de la respuesta
    const responseVoice = await elevenlabs.generateSpeech(
      response.responseText,
      voiceConfig.voiceId,
      {
        currentEmotion: response.metadata.emotionsTriggered[0] || "neutral",
        intensity: 0.5,
        mood: {
          valence: 0.3, // Mood mÃ¡s serio por el contexto
          arousal: 0.4,
          dominance: 0.5,
        },
        stability: 0.5,
        similarity_boost: 0.75,
      }
    );

    const responseAudioPath = path.join(outputDir, "test-voice-response.mp3");
    fs.writeFileSync(responseAudioPath, responseVoice.audioBuffer);

    console.log(`   âœ… Response voice generated:`);
    console.log(`      Audio size: ${responseVoice.audioBuffer.length} bytes`);
    console.log(`      Emotions: ${response.metadata.emotionsTriggered.join(", ")}`);
    console.log(`   ğŸ’¾ Response audio saved to: ${responseAudioPath}\n`);

    // 8. Verify voice config stats
    console.log("8ï¸âƒ£  Verifying voice usage statistics...");

    const updatedConfig = await getVoiceConfig(agentId);

    console.log(`   âœ… Usage statistics:`);
    console.log(`      Total generations: ${updatedConfig?.totalVoiceGenerations}`);
    console.log(`      Total transcriptions: ${updatedConfig?.totalTranscriptions}\n`);

    // 9. Test voice characteristics mapping
    console.log("9ï¸âƒ£  Testing personality â†’ voice characteristics mapping...");

    const agent = await prisma.agent.findUnique({
      where: { id: agentId },
      include: {
        personalityCore: true,
      },
    });

    if (agent?.personalityCore) {
      console.log(`   âœ… Personality traits:`);
      console.log(`      Openness: ${agent.personalityCore.openness}/100`);
      console.log(`      Conscientiousness: ${agent.personalityCore.conscientiousness}/100`);
      console.log(`      Extraversion: ${agent.personalityCore.extraversion}/100`);
      console.log(`      Agreeableness: ${agent.personalityCore.agreeableness}/100`);
      console.log(`      Neuroticism: ${agent.personalityCore.neuroticism}/100\n`);

      console.log(`   ğŸ¯ Mapped to voice:`);
      console.log(`      High agreeableness (${agent.personalityCore.agreeableness}) â†’ Warm, empathetic voice`);
      console.log(`      High extraversion (${agent.personalityCore.extraversion}) â†’ Energetic, expressive voice`);
    }

    const totalTime = Date.now() - startTime;

    console.log("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    console.log(`âœ… VOICE SYSTEM TEST COMPLETE (${totalTime}ms)`);
    console.log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    console.log("ğŸ‰ ALL VOICE SYSTEMS OPERATIONAL:");
    console.log("   âœ… Automatic voice selection from ElevenLabs library");
    console.log("   âœ… Voice configuration per agent");
    console.log("   âœ… Personality-based voice matching");
    console.log("   âœ… Emotional modulation of voice");
    console.log("   âœ… Text-to-Speech generation");
    console.log("   âœ… Voice usage statistics tracking");
    console.log("   âœ… Audio file export\n");

    console.log("ğŸ“ NEXT STEPS:");
    console.log("   1. Test Whisper transcription with real audio");
    console.log("   2. Test via API endpoint: POST /api/chat/voice");
    console.log("   3. Build frontend voice chat UI");
    console.log("   4. Add voice streaming for lower latency\n");

    console.log("ğŸ§ AUDIO FILES GENERATED:");
    console.log(`   â€¢ ${audioPath}`);
    console.log(`   â€¢ ${responseAudioPath}`);
    console.log("\n   Play these files to hear Sofia's voice!\n");

  } catch (error) {
    console.error("\nâŒ TEST FAILED:", error);
    throw error;
  } finally {
    await prisma.$disconnect();
  }
}

// Run test
testVoiceSystem()
  .then(() => {
    console.log("\nâœ… Test completed successfully\n");
    process.exit(0);
  })
  .catch((error) => {
    console.error("\nâŒ Test failed with error:", error);
    process.exit(1);
  });
